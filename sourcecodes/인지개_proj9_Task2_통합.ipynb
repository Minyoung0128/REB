{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SWE3011_41 Task2\n"
      ],
      "metadata": {
        "id": "qFDsP7Tpudfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Engineering with Langchain for LLMS**\n",
        "\n",
        "1. Based on the given code, you need to extend (modify) to a classification task using Langchain.\n",
        "2. Since large language model baseline is required for the task, you may choose any model from OpenAI API and HuggingfaceHub.\n",
        "\n",
        " **OpenAI**: Only available for paid users.\n",
        "\n",
        " **HuggingfaceHUB**: Free to use with usage limits (reset hourly).\n",
        "\n",
        "3. Conduct experiments and document the results in the report. Here, you should consider what kind of prompt design you use so please find some tutorials/resources in our homework description to obtain more information.\n"
      ],
      "metadata": {
        "id": "-068fzxmWi-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installation**"
      ],
      "metadata": {
        "id": "I03lMSpzuehA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "N3NPPty0zRig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --force-reinstall openai\n"
      ],
      "metadata": {
        "id": "sFlYDnWqsOb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
        "HUGGINGFACEHUB_API_TOKEN = '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'\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n"
      ],
      "metadata": {
        "id": "yQjLnvhhIh3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install huggingface_hub"
      ],
      "metadata": {
        "id": "2zRELEImtc7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1e16d1-a4be-4fe1-9612-463e934b4477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "id": "XUmH2mQ1ryhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "metadata": {
        "id": "kBReoKrJrHhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "def evaluate_model_nlp(y_pred, y_test):\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "#evaluate_model(y_pred, y_test)"
      ],
      "metadata": {
        "id": "SHWXKT1xrgIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Load Dataset**"
      ],
      "metadata": {
        "id": "7j5DoyMyvD-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Evaluation should be done using **provided test dataset**"
      ],
      "metadata": {
        "id": "ikhnKhXGG4hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# You can use train_ds for few-shot examples\n",
        "train_ds = load_dataset(\"glue\", \"sst2\", split=\"train\")\n",
        "\n",
        "# Evaluation should be done using test_ds\n",
        "# test_ds = load_dataset(\"csv\", data_files=\"./test_dataset.csv\")['train']"
      ],
      "metadata": {
        "id": "0jhZ-TMWvcxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = load_dataset(\"csv\", data_files=\"./test_dataset.csv\")['train']"
      ],
      "metadata": {
        "id": "ceGZuO3KFC_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Preparing Prompt**"
      ],
      "metadata": {
        "id": "myBeOd4LvFbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[123]['sentence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "m557YN2Rnjig",
        "outputId": "a2e434ad-be83-421c-e4a9-c45aff689c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'proves a lovely trifle that , unfortunately , is a little too in love with its own cuteness . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds1 = load_dataset(\"csv\", data_files=\"./responses.csv\")['train']"
      ],
      "metadata": {
        "id": "cBhGB0BW9XRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Edit Prompt Freely\n",
        "template = \"\"\"Question: {question}\\nAnswer : \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template= template,\n",
        "    input_variables=[\"question\"]\n",
        ")\n",
        "\n",
        "question = \"When was the FIFA World Cup first held? \""
      ],
      "metadata": {
        "id": "_jzAqjN_0t9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Inference**"
      ],
      "metadata": {
        "id": "dTVjHx6q0rAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub, OpenAI\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "ScDyg3aqvdQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_llm(model_name, api_key=None, openai=False):\n",
        "    \"\"\"\n",
        "    Initialize the model using the langchain library.\n",
        "    \"\"\"\n",
        "    if openai:\n",
        "      llm = OpenAI(model_name=model_name, openai_api_key=api_key)\n",
        "    else:\n",
        "      llm = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\": 0.22, \"max_length\": 0.1})\n",
        "\n",
        "    return llm"
      ],
      "metadata": {
        "id": "6L_2Zuy4vrQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interaction(llm, prompt, question, openai=False):\n",
        "    \"\"\"\n",
        "    Use a templated prompt to get a response from the LLM.\n",
        "    \"\"\"\n",
        "    if openai:\n",
        "      final_prompt = prompt.format(question=question)\n",
        "      response = llm(final_prompt)\n",
        "    else:\n",
        "      llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "      response = llm_chain.run(question)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "kKHT5lanwIXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Use OpenAI Model\n",
        "# model_name = \"text-davinci-003\"\n",
        "# openai_api_key = \"OPENAI_API_KEY\" # Make sure to replace with your actual key\n",
        "\n",
        "# llm = initialize_llm(model_name, api_key=openai_api_key, openai=True)\n",
        "# response = interaction(llm, prompt, question, openai=True)\n",
        "# print(f\"LLM Output: {response}\")"
      ],
      "metadata": {
        "id": "oLABzozuvxpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To Use HuggingfaceHUB Model\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "question = f\"\"\"\n",
        "  I'll give you a sentence.\n",
        "  Change this sentence to be very specific, long and detailed so that the emotion or content is revealed much better.\n",
        "  {test_ds[40]['sentence']}\n",
        "\"\"\"\n",
        "response = interaction(llm, prompt, question, openai=False)\n",
        "print(f\"LLM Output: {response}\")"
      ],
      "metadata": {
        "id": "LU_YBMynx8PR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc2423d-ddeb-4145-c49a-f886ee602111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Output: while undisputed isn't exactly a high, it is a gripping, tidy little movie that takes mr. hill higher than he's been in a while.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds[40]['sentence']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "aI9dXrZp_Mk4",
        "outputId": "b0b613b0-16d8-4378-e1f7-7ae60e6edb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"while undisputed is n't exactly a high , it is a gripping , tidy little movie that takes mr. hill higher than he 's been in a while . \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate_model_nlp(y_pred, y_test)"
      ],
      "metadata": {
        "id": "d9a4wdIlsRZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## zero-shot case 94, 93"
      ],
      "metadata": {
        "id": "RhNDQg_rKpxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "match_count = 0\n",
        "y_list = []\n",
        "p_list = []\n",
        "for i in range(100):\n",
        "  question =  'Classify whether the following sentence is positive or negative. \\n If positive, print 1. If negative, print 0.\\n text : ' + test_ds[i]['sentence']\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  if (str(response) == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "\n",
        "evaluate_model_nlp(y_list, p_list)\n"
      ],
      "metadata": {
        "id": "p-Qb5CU7Bz-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6032906-08f1-4316-c0ba-be96ed40a1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n",
            "Accuracy: 0.94\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.91      0.95        58\n",
            "           1       0.89      0.98      0.93        42\n",
            "\n",
            "    accuracy                           0.94       100\n",
            "   macro avg       0.94      0.94      0.94       100\n",
            "weighted avg       0.94      0.94      0.94       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "match_count = 0\n",
        "y_list = []\n",
        "p_list = []\n",
        "for i in range(100):\n",
        "  question = \"considering this conext, determine if the following sentence is more likely to be positive, print 1, if negative, print 0\\n \" + test_ds[i]['sentence']\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  if (str(response) == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "evaluate_model_nlp(y_list, p_list)\n",
        "print(match_count)"
      ],
      "metadata": {
        "id": "B1V3wjCxLG7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ed9a82-cf44-4afe-86a3-9be876452e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.93\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.94        55\n",
            "           1       0.91      0.93      0.92        45\n",
            "\n",
            "    accuracy                           0.93       100\n",
            "   macro avg       0.93      0.93      0.93       100\n",
            "weighted avg       0.93      0.93      0.93       100\n",
            "\n",
            "93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one-shot case : 95"
      ],
      "metadata": {
        "id": "uTU04aSyLHJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "match_count = 0\n",
        "y_list = []\n",
        "p_list = []\n",
        "for i in range(100):\n",
        "  question =  \"\"\"Classify whether the following sentence is positive or negative.\n",
        "   If positive, print 1. If negative, print 0.\n",
        "    i give one example.\n",
        "    text : \"\"\" + train_ds[1]['sentence'] + \"\\n answer : \" + str(train_ds[1]['label']) + '\\n text : ' + test_ds[i]['sentence'] +'answer : '\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  if (str(response) == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "evaluate_model_nlp(y_list, p_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "YLFUlqDnLHDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b08149-3ddd-4f54-d536-83896cc50dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95\n",
            "Accuracy: 0.95\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.95        55\n",
            "           1       0.93      0.96      0.95        45\n",
            "\n",
            "    accuracy                           0.95       100\n",
            "   macro avg       0.95      0.95      0.95       100\n",
            "weighted avg       0.95      0.95      0.95       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot 3개의 예시  결과 :  92\n"
      ],
      "metadata": {
        "id": "ebMXmhIcN3sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "y_list = []\n",
        "p_list = []\n",
        "match_count = 0\n",
        "for i in range(100):\n",
        "  question = 'if sentence is positive, print 1, if negative, print 0. i give 3 test example.\\n' + train_ds[1]['sentence'] + ' : ' + str(train_ds[1]['label']) + '\\n' + train_ds[2]['sentence'] + ' : ' + str(train_ds[2]['label']) + '\\n' + train_ds[3]['sentence'] + ' : ' + str(train_ds[3]['label']) + '\\n' + test_ds[i]['sentence'] + '\\n'\n",
        "\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  if (str(response) == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "evaluate_model_nlp(y_list, p_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "OLNeZcrsLHBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e3ff15-9265-4631-8c5a-6186eefea973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92\n",
            "Accuracy: 0.92\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.93        56\n",
            "           1       0.89      0.93      0.91        44\n",
            "\n",
            "    accuracy                           0.92       100\n",
            "   macro avg       0.92      0.92      0.92       100\n",
            "weighted avg       0.92      0.92      0.92       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## few-shot 9개의 예시  결과 :  91\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ydLcjEpqRj4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "y_list = []\n",
        "p_list = []\n",
        "match_count = 0\n",
        "for i in range(100):\n",
        "  question = 'if sentence is positive, print 1, if negative, print 0. i give 9 test example.\\n' + train_ds[1]['sentence'] + ' : ' + str(train_ds[1]['label']) + '\\n' + train_ds[2]['sentence'] + ' : ' + str(train_ds[2]['label']) + '\\n' + train_ds[3]['sentence'] + ' : ' + str(train_ds[3]['label'])+ '\\n' + train_ds[4]['sentence'] + ' : ' + str(train_ds[4]['label']) + '\\n' + train_ds[5]['sentence'] + ' : ' + str(train_ds[5]['label'])+ '\\n' + train_ds[6]['sentence'] + ' : ' + str(train_ds[6]['label'])+ '\\n' + train_ds[7]['sentence'] + ' : ' + str(train_ds[7]['label'])+ '\\n' + train_ds[8]['sentence'] + ' : ' + str(train_ds[8]['label'])+ '\\n' + train_ds[9]['sentence'] + ' : ' + str(train_ds[9]['label'])+ '\\n' + test_ds[i]['sentence'] + '\\n'\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  if (str(response) == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "evaluate_model_nlp(y_list, p_list)\n"
      ],
      "metadata": {
        "id": "jFseFIx-LG_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512fc503-c0e9-44f0-effd-9b4996597cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91\n",
            "Accuracy: 0.91\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92        61\n",
            "           1       0.83      0.97      0.89        39\n",
            "\n",
            "    accuracy                           0.91       100\n",
            "   macro avg       0.90      0.92      0.91       100\n",
            "weighted avg       0.92      0.91      0.91       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain-of-Thought : 95"
      ],
      "metadata": {
        "id": "wKT9KZ0xUQrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "y_list = []\n",
        "p_list = []\n",
        "match_count = 0\n",
        "for i in range(100):\n",
        "  question = \"\"\"\n",
        "  Let's classify positive and negative sentences.\n",
        "  If the sentence is positive, 1 is output, if it is negative, 0 is output.\n",
        "  Q: the part where nothing’s happening\n",
        "  A: This sentence suggests boredom. Also there are no indicators indicating positivity: 0\n",
        "  Q: {}\n",
        "  A:\n",
        "  \"\"\".format(test_ds[i]['sentence'])\n",
        "\n",
        "\n",
        "\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "  #print(response + ' : ' + str(test_ds[i]['label']))\n",
        "  if (str(response)[-1] == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "evaluate_model_nlp(y_list, p_list)"
      ],
      "metadata": {
        "id": "jKKxpdyALG5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1740764f-ee35-4370-cdb6-ed9875240658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95\n",
            "Accuracy: 0.95\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.95        55\n",
            "           1       0.93      0.96      0.95        45\n",
            "\n",
            "    accuracy                           0.95       100\n",
            "   macro avg       0.95      0.95      0.95       100\n",
            "weighted avg       0.95      0.95      0.95       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##트리거 문장\"let's think step by step\"을 포함한 CoT : 90"
      ],
      "metadata": {
        "id": "B5wJ1ISSkx9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "match_count = 0\n",
        "y_list = []\n",
        "p_list = []\n",
        "for i in range(100):\n",
        "  question = \"\"\"\n",
        "  Let's classify positive(1) and negative(0) sentences.\n",
        "  let's think step by step\n",
        "  Q: the part where nothing’s happening\n",
        "  A: This sentence suggests boredom. Also there are no indicators indicating positivity: 0\n",
        "  Q: {}\n",
        "  A:\n",
        "  \"\"\".format(test_ds[i]['sentence'])\n",
        "\n",
        "  response = interaction(llm, prompt, question, openai=False)\n",
        "  y_list.append(int(test_ds[i]['label']))\n",
        "  p_list.append(int(response))\n",
        "\n",
        "  if (str(response)[-1] == str(test_ds[i]['label'])):\n",
        "      match_count+= 1\n",
        "\n",
        "print(match_count)\n",
        "evaluate_model_nlp(y_list, p_list)"
      ],
      "metadata": {
        "id": "aWKC8VVzLG22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b173ae0-351b-41e5-9ec4-17a327681640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "Accuracy: 0.90\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.85      0.91        62\n",
            "           1       0.80      0.97      0.88        38\n",
            "\n",
            "    accuracy                           0.90       100\n",
            "   macro avg       0.89      0.91      0.90       100\n",
            "weighted avg       0.91      0.90      0.90       100\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Clarification\n",
        "\n"
      ],
      "metadata": {
        "id": "DEPUKrJBlmBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기존 모호한 문장을 이해하기 쉬운 문장으로 변환 후 해당 문장들을 bert model에 전달함."
      ],
      "metadata": {
        "id": "AgZOlSe29nQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V1"
      ],
      "metadata": {
        "id": "B_WffSse1zN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "\n",
        "\n",
        "with open('responses12.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Sentence', 'Response', 'Target'])\n",
        "\n",
        "\n",
        "    for i in range(100):\n",
        "        question = f\"make this sentence \\\"{test_ds[i]['sentence']}\\\" more understandable\"\n",
        "        response = interaction(llm, prompt, question, openai=False)\n",
        "        target = str(test_ds[i]['label'])\n",
        "        writer.writerow([test_ds[i]['sentence'], response, target])\n"
      ],
      "metadata": {
        "id": "pNI6zOlgxMJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2"
      ],
      "metadata": {
        "id": "h7DPFw2J12rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "\n",
        "\n",
        "with open('responses2.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Response', 'Target'])\n",
        "\n",
        "\n",
        "    for i in range(100):\n",
        "        question = f\"If it is difficult to judge whether the following sentence is positive or negative, convert it into a sentence that is easier to judge. If you can clearly judge it, print the existing sentence.\\n{test_ds[i]['sentence']}\"\n",
        "        response = interaction(llm, prompt, question, openai=False)\n",
        "        target = str(test_ds[i]['label'])\n",
        "        writer.writerow([response, target])\n"
      ],
      "metadata": {
        "id": "7rxN41O692au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V3"
      ],
      "metadata": {
        "id": "fcrfAzCV15T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "\n",
        "\n",
        "with open('responses17.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['sentence', 'label'])\n",
        "\n",
        "\n",
        "    for i in range(100):\n",
        "        question = f\"\"\"\n",
        "        think step by step.\n",
        "        You are working on converting the sentences into simpler sentences so that the positives and negatives are more evident.\n",
        "        If it contains a proverb or idiom, convert it to make it easier to understand.\n",
        "        If there is an element of reversal, such as but or although, emphasize that part.\n",
        "        And you must change the sentence in easy word.\n",
        "        I'll give you three examples.\n",
        "        example 1 : original : {train_ds[10101]['sentence']}   converted : It is clear that the filmmakers have good intentions, but despite this, the film does not achieve the desired results and in fact has the opposite effect.\n",
        "        example 2 : original : {train_ds[200]['sentence']}   converted : told in haphazard fashion\n",
        "        example 3 : original : {train_ds[25]['sentence']}   converted : Enhanced by a brilliantly diverse cast of exuberant and whimsical characters, each bringing their own unique flair and creativity to the ensemble, creating an exceptionally vibrant and captivating experience.\n",
        "\n",
        "        original : {test_ds[i]['sentence']}.  converted :\n",
        "        \"\"\"\n",
        "        response = interaction(llm, prompt, question, openai=False)\n",
        "        target = str(test_ds[i]['label'])\n",
        "        writer.writerow([response, target])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueehuicliH4N",
        "outputId": "56e136a8-f881-49db-9c07-58ca505dc9f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V4"
      ],
      "metadata": {
        "id": "Lry0mraD2FOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "model_name = \"google/flan-t5-xxl\"\n",
        "llm = initialize_llm(model_name, openai=False)\n",
        "\n",
        "\n",
        "with open('responses21.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['sentence', 'label'])\n",
        "\n",
        "    for i in range(100):\n",
        "        question = f\"\"\"\n",
        "        think step by step.\n",
        "        I'll give you a sentence.\n",
        "        Change below sentence to be very specific, long and detailed so that the semantic or content is revealed much better. and explain it\n",
        "        {test_ds[i]['sentence']}\n",
        "        \"\"\"\n",
        "        response = interaction(llm, prompt, question, openai=False)\n",
        "        target = str(test_ds[i]['label'])\n",
        "        writer.writerow([response, target])\n"
      ],
      "metadata": {
        "id": "z4XTjjyb_Z1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}